{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f2b91cbd82bc39d047f3f92d7439f4985b3a1246",
        "id": "UVFZi3nlfwkT"
      },
      "source": [
        "## **Instituto de Informática - UFRGS**\n",
        "## Disciplina INF01017 - Aprendizado de Máquina\n",
        "#### *Profa. Mariana Recamonde-Mendoza (mrmendoza@inf.ufrgs.br)*\n",
        "### **Trabalho 2 - Análise de agrupamentos com K-means**\n",
        "<br> \n",
        "\n",
        "**Grupo**:\n",
        "Giovani da Silva - 00305086\n",
        "\n",
        "*** Explicações estão contidas no relatório em anexo ***\n",
        "\n",
        "\n",
        "---\n",
        "***Observação:*** *Este notebook é disponibilizado aos alunos como ponto de partida para o desenvolvimento do trabalho prático 2 (T2) da disciplina INF01017. Os alunos podem optar por fazer o download dos dados e realizar a análise em outro notebook (fora do Google Colab) ou outro software. Entretanto, o grupo deve se atentar em discutir os aspectos solicitados a respeito da análise de agrupamentos e interpretação dos resultados.*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "***ENTREGA:*** A entrega deste trabalho deve ser feita enviando o **link do Google Colab** com a solução do grupo, com a opção de deixar **as saídas do notebook salvas**. Alternativamente, os grupos podem enviar o Google Colab (ou script Python) e o relatório em PDF com os resultados das análises e a interpretação dos resultados.\n",
        "\n",
        "\n",
        "\n",
        "# Segmentação de Clientes com Algoritmo K-Means\n",
        "\n",
        "O conjunto de dados a ser utilizado nesse trabalho foi adaptado de um problema de classificação de risco de crédito para clientes bancários. Os dados a serem analisados não possuem rótulos (classes) e possuem apenas um subconjunto previamente selecionado dos atributos utilizados para descrever as instâncias analisadas. O objetivo é realizar o processo de segmentação de clientes que solicitaram crédito bancário, que consiste em separar os clientes em grupos menores com base em características comuns entre eles. A partir destes grupos gerados (aqui denominados clusters), a empresa pode oferecer uma comunicação mais assertiva e personalizada aos seus clientes, e melhor compreender sobre os interesses e necessidades dos seus clientes ao traçar perfis de clientes (também denominados *personas*).\n",
        "\n",
        "\n",
        "Os atributos disponíveis estão descritos abaixo:\n",
        "\n",
        "*   **Age.** Idade (numérico)\n",
        "*   **Credit amount.** Valor do crédito (numérico, em DM - Deutsch Mark)\n",
        "*   **Duration.** Duração (numérico, em mês)\n",
        "*   **Sex.** Sexo (categórico: masculino, feminino)\n",
        "*   **Job.** Emprego (categórico: 0 - não qualificado e não residente, 1 - não qualificado e residente, 2 - qualificado, 3 - altamente qualificado)\n",
        "*   **Housing.** Imóvel (categórico: próprio, alugado ou gratuito)\n",
        "*   **Saving Account.** Poupança (categórico: pequena, moderada, bastante rico, rico)\n",
        "*   **Checking Account.** Conta corrente (categórico: pequena, moderada, bastante rico, rico)\n",
        "*   **Purpose.** Finalidade (categórico: carro, móveis/equipamentos, rádio/TV, eletrodomésticos, reparos, educação, negócios, férias/outros)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "yaEkuBIVfwkX"
      },
      "outputs": [],
      "source": [
        "##importando bilbiotecas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5b0ddca052b76bd09dd76022b83f3f2e91730863",
        "id": "czlOlcPcfwkZ"
      },
      "source": [
        "Lendo os dados e visualizando a estrutura para as primeiras instâncias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "0cfe8b507927d2e3cedb1a524b77b8a614e3361e",
        "id": "Z7IYOrT6fwkZ"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"https://drive.google.com/uc?export=view&id=1Jp-y1djRI3sCT6_JTBkfLn1FOL4qMHsv\",  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "bfbb4dd2ee83304d6608a57e69dad7830f872e78",
        "id": "ZDoo9nOAfwka"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A primeira coluna pode ser removida, pois é apenas um identificador (da instância ou da linha)."
      ],
      "metadata": {
        "id": "srD9ogm8sMbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(data.columns[0], inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "Oy0lh1jokqB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecionando o tamanho do base de dados, os tipos dos atributos e a ocorrência de valores faltantes."
      ],
      "metadata": {
        "id": "TovjnuFgsUwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"O conjunto de dados possui {} instâncias (clientes) e {} colunas (atributos).\".format(data.shape[0],data.shape[1]))"
      ],
      "metadata": {
        "id": "6wDJxuYBsSt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Valores faltantes (%) por atributo:\\n{}\".format((data.isnull().sum()/data.shape[0])*100))"
      ],
      "metadata": {
        "id": "s-vqHFB_shaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os valores faltantes ocorrem nos atributos Saving accounts e Checking account. Provavelmente são clientes que não possuem algum destes tipos de conta."
      ],
      "metadata": {
        "id": "8hlA8CBZwGRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tipo de dado por atributo:\\n{}\".format(data.dtypes))"
      ],
      "metadata": {
        "id": "Lzvd-m5EstUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1f7e050b147b3e851a89e535848e8ce5ebf287a8",
        "id": "xEGN0Ht_fwkZ"
      },
      "source": [
        "Embora o atributo Job esteja codificado como inteiro, ele possui uma interpretação **categórica**. Vamos fazer a conversão de tipo e separar os atributos em vetores de categóricos e numéricos para facilitar a análise exploratória dos dados."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Converte o atributo job para object.\n",
        "data_types_dict = {'Job': object}\n",
        "\n",
        "data = data.astype(data_types_dict)\n",
        "\n",
        "## Separa os atributos em vetores, de acordo com o tipo de dado (categórico ou numérico)\n",
        "cat_columns=list(data.select_dtypes(include=[\"object\"]).columns)\n",
        "print(cat_columns)\n",
        "\n",
        "num_columns=list(data.select_dtypes(include=[\"int64\", \"float64\"]).columns)\n",
        "print(num_columns)\n",
        "\n",
        "## Separa os dados em dois dataframes, de atributos numéricos e categóricos\n",
        "data_num = data[num_columns]\n",
        "data_cat = data[cat_columns]"
      ],
      "metadata": {
        "id": "6hcQ64iDutWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_num.describe()"
      ],
      "metadata": {
        "id": "b5amPQ2Ukq5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A análise de agrupamento para identificar perfis de clientes será feita a partir dos dados numéricos: **Age, Credit Amount, Duration**.\n",
        "\n",
        "\n",
        "Uma vez definidos os clusters, os demais atributos serão empregados para uma interpretação mais aprofundada destes clusters e dos respectivos perfis de clientes que representam.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_KXmWdDVwqBK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "67e3ec17ce4b5ea2f809c9f52b27849130a1ce53",
        "id": "r8T43EHWfwkf"
      },
      "source": [
        "### **Análise Exploratória dos Dados**\n",
        "\n",
        "Nas células abaixo, vamos realizar uma análise exploratória dos dados. \n",
        "\n",
        "Primeiramente, vamos observar a relação entre os três atributos numéricos através de um gráfico 3D e de gráficos que traçam a relação par a par entre eles."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D \n",
        "fig = plt.figure(figsize=(12,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(data[\"Credit amount\"], data[\"Duration\"], data[\"Age\"])\n",
        "ax.set_xlabel(\"Credit amount\")\n",
        "ax.set_ylabel(\"Duration\")\n",
        "ax.set_zlabel(\"Age\")"
      ],
      "metadata": {
        "id": "72FDfQ5fxvRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "b6d7ebe749a9a53012ba606e874f9a392394d336",
        "id": "x13c4GXYfwkf"
      },
      "outputs": [],
      "source": [
        "def scatters(data, h=None, pal=None):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(8,8))\n",
        "    sns.scatterplot(x=\"Credit amount\",y=\"Duration\", hue=h, palette=pal, data=data, ax=ax1)\n",
        "    sns.scatterplot(x=\"Age\",y=\"Credit amount\", hue=h, palette=pal, data=data, ax=ax2)\n",
        "    sns.scatterplot(x=\"Age\",y=\"Duration\", hue=h, palette=pal, data=data, ax=ax3)\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "ba9afedfd8b250ca725d9e93bfccd4b393f90a0c",
        "id": "nZo5s2oNfwkg"
      },
      "outputs": [],
      "source": [
        "scatters(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estas análises sugerem a existência de uma correlação positiva entre Credit amount e Duration. Esta correlação é pertinente, visto que valores maiores de créditos tendem a ser pagos em um prazo maior."
      ],
      "metadata": {
        "id": "keX6mR6PIu10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos observar, também, a distribuição de cada atributo numérico. "
      ],
      "metadata": {
        "id": "InjqWeXP5Mw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distributions(df):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(8,8))\n",
        "    sns.distplot(df[\"Age\"], ax=ax1)\n",
        "    sns.distplot(df[\"Credit amount\"], ax=ax2)\n",
        "    sns.distplot(df[\"Duration\"], ax=ax3)\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "KylwLCeA5LsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distributions(data_num)"
      ],
      "metadata": {
        "id": "s5is7lSL5U7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com a análise das distribuições, percebemos de forma ainda mais clara que os atributos variam em escalas diferentes. Em algoritmos que lidam com medidas de proximidade, como é o caso do k-means, é importante normalizar os dados para que os valores dos diferentes atributos estejam em ordens de grandeza similares, e assim exerçam o mesmo impacto no aprendizado. Iremos utilizar o método StandardScaler (também chamado por padronização)."
      ],
      "metadata": {
        "id": "pi3Tv6qn1--9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "data_num_std = scaler.fit_transform(data_num)\n",
        "data_num_std = pd.DataFrame(data_num_std, columns=num_columns)"
      ],
      "metadata": {
        "id": "Qs-dunC8ywZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Visualizando novamente os dados, após padronização \n",
        "## (Apenas para entender, visualmente, que o padrão na relação dos dados não muda)\n",
        "scatters(data_num_std)"
      ],
      "metadata": {
        "id": "vq0pUW_L15nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(data_num_std[\"Credit amount\"], data_num_std[\"Duration\"], data_num_std[\"Age\"])\n",
        "ax.set_xlabel(\"Credit amount\")\n",
        "ax.set_ylabel(\"Duration\")\n",
        "ax.set_zlabel(\"Age\")"
      ],
      "metadata": {
        "id": "Umjrh_zp2lvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Aplicação do K-means**"
      ],
      "metadata": {
        "id": "KXi-M_d-6MhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta seção, o grupo deve realizar a aplicação do algoritmo [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) no dataframe `data_num_std`, seguindo os passos listados abaixo:\n",
        "\n",
        "\n",
        "1.   Faça a análise do k-means para diferentes números de clusters. Sugere-se testar de 1 a 20. Crie um vetor para armazenar a soma do quadrado das distâncias das instâncias até o centróide mais próximo durante o loop (também chamado de inércia, e disponível no campo *inertia_* do objeto retornado pelo método KMeans, por exemplo, kmeans.inertia_)  \n",
        "2.   Faça um gráfico da inércia (eixo y) para os diferentes valores de k (eixo x), a fim de determinar o melhor valor de k pelo método do cotovelo (Elbow method).\n",
        "3.   Escolha o melhor valor de k para os dados, repetindo a execução do k-means com o k escolhido e gerando a configuração final de clusters.\n",
        "\n"
      ],
      "metadata": {
        "id": "b_LfIhVM6go4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##1\n",
        "max_clusters = 20 #@param {type:\"integer\"}\n",
        "inertias=[]\n",
        "for ii in range(1,max_clusters):\n",
        "    kmeans = KMeans(n_clusters=ii, random_state=42)\n",
        "    kmeans.fit(data_num_std)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "print(inertias)"
      ],
      "metadata": {
        "id": "UucVYGWi-6oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##2\n",
        "plt.figure()\n",
        "plt.plot(range(1,max_clusters),inertias, marker='o')"
      ],
      "metadata": {
        "id": "apRYzpUCzmvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "silhouette_scores = []\n",
        "X = data_num_std\n",
        "for i in range(2, 10):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "    kmeans.fit(X)\n",
        "    labels = kmeans.labels_\n",
        "    silhouette_avg = silhouette_score(X, labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "print(silhouette_scores)\n",
        "\n",
        "# Plot the silhouette scores\n",
        "plt.plot(range(2, 10), silhouette_scores)\n",
        "plt.title('Silhouette Analysis')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aNoFJvro5J1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source code: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "\n",
        "# Generating the sample data from make_blobs\n",
        "# This particular setting has one distinct cluster and 3 clusters placed close\n",
        "# together.\n",
        "X = data_num_std\n",
        "range_n_clusters = [2,3,4,5,6]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\n",
        "        \"For n_clusters =\",\n",
        "        n_clusters,\n",
        "        \"The average silhouette_score is :\",\n",
        "        silhouette_avg,\n",
        "    )\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(\n",
        "            np.arange(y_lower, y_upper),\n",
        "            0,\n",
        "            ith_cluster_silhouette_values,\n",
        "            facecolor=color,\n",
        "            edgecolor=color,\n",
        "            alpha=0.7,\n",
        "        )\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"n_clusters = %d\"\n",
        "        % n_clusters)\n",
        "    \n",
        "\n",
        "    ax1.set_xlabel(\"Valores de coeficiente da silhueta\")\n",
        "    ax1.set_ylabel(\"Cluster\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        " \n",
        "\n",
        "    plt.suptitle(\n",
        "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
        "        % n_clusters,\n",
        "        fontsize=14,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gVm0CPkM1oh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#não esta muito claro onde é o ponto de cutuvelo, parece ser entre 3 e 4\n",
        "\n",
        "num_clusters = 3 #@param {type:\"integer\"}\n",
        "\n",
        "kmeans_final = KMeans(num_clusters )\n",
        "kmeans_final.fit(data_num_std)\n",
        "labels=kmeans_final.labels_"
      ],
      "metadata": {
        "id": "fInqdjJLzv5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A célula abaixo gera uma versão do conjunto de dados com uma coluna 'cluster' adicionada aos dados, indicando o índice do cluster ao qual foi designada cada instância. Esta versão será útil para interpretação dos resultados, buscando analisar padrões e tendências por cluster."
      ],
      "metadata": {
        "id": "D0Fw4bA0HNv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusters_config = pd.concat([data_num_std, data_cat, pd.DataFrame({'cluster':labels})], axis=1)\n",
        "clusters_config.head()"
      ],
      "metadata": {
        "id": "cHSJZtkezxIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualização do resultado do agrupamento**\n",
        "\n",
        "Para visualizar o resultado do agrupamento em um gráfico de 2D, vamos usar a estratégia de *Principal Component Analysis* (PCA), que faz uma projeção dos dados a partir da combinação linear dos atributos (dimensões originais).\n",
        "\n",
        "Esta transformação será realizada tanto nos dados usados no algoritmo k-means, como nos centróides dos clusters gerados pelo algoritmo.\n",
        "\n",
        "Cada ponto será representado pelas coordenadas {PC1, PC2} (onde PC = *Principal Component*) e a cor do ponto no gráfico corresponde ao seu respectivo cluster. "
      ],
      "metadata": {
        "id": "G_fUTZnYHdsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca_2 = PCA(2)\n",
        "pca_2_result = pca_2.fit_transform(data_num_std)\n",
        "\n",
        "## obtém os centrois do k-means e aplica a transformação por PCA\n",
        "centroids = kmeans_final.cluster_centers_\n",
        "centroids_pca = pca_2.transform(centroids)\n",
        "\n",
        "## plota a figura, colorindo os pontos de acordo com o respectivo cluster.\n",
        "sns.set(style='white', rc={'figure.figsize':(9,6)},font_scale=1.1)\n",
        "\n",
        "plt.scatter(x=pca_2_result[:, 0], y=pca_2_result[:, 1], c=labels, cmap='viridis')\n",
        "plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1],\n",
        "            marker='x', s=169, linewidths=3,\n",
        "            color='black', zorder=10,lw=3)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('Clustered Data (PCA visualization)',fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "soqwCpTc92Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Interpretação dos resultados**"
      ],
      "metadata": {
        "id": "gRLiv9GUIDoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A análise de agrupamentos tem como resultado a geração de **clusters** que são definidos com base na **alta similaridades entre as características** (ou atributos) **das instâncias**. Assim, usualmente o resultado do agrupamento precisa ser explorado visualmente, através de gráficos, tabelas ou estatística descritiva, e interpretado.\n",
        "\n",
        "Nesta seção do documento, os grupos deverão implementar suas estratégias para **analisar e comparar** a distribuição dos atributos entre os clusters encontrados. **O objetivo é traçar um perfil dos clientes que se encaixa em cada cluster obtido, com base nos atributos disponíveis** (numéricos e categóricos, incluindo aqueles que não foram usados para realizar o agrupamento). Ao final da análise, os grupos devem ser capazes de realizar uma descrição de cada cluster em termos do perfil de cliente que ele melhor representa.\n",
        "\n",
        "Por exemplo... Algum cluster está associado com pessoas mais jovens? Como está distribuído o valor de crédito em cada cluster? Existe uma proporção maior de mulheres em algum cluster? Existe alguma relação entre os clusters e o tipo de emprego, de moradia, ou de contas (poupança e correnta)? etc. Os grupos podem (e devem) usar a criatividade para realizar a análise dos dados e a interpretação dos dados. \n",
        "\n",
        "Sugere-se uso de recursos como média e desvio padrão, gráfico de barras, graficos de boxplot, histogramas, ou outros apropriados para análise de distribuição (de acordo com o tipo de atributo, numérico ou categórico).\n",
        " \n",
        "Os resultados devem ser apresentados e discutidos no próprio notebook do Google Colab ou, alternativamente, em relatório em PDF a ser entregue junto com o notebook.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NnHVBYNEMuHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = clusters_config\n",
        "# Seleciona apenas as colunas relevantes\n",
        "num_cols = ['Credit amount', 'Age', 'Duration','cluster']\n",
        "df_num = df[num_cols]\n",
        "\n",
        "\n",
        "\n",
        "# Agrupa os dados pelos clusters\n",
        "grouped = df_num.groupby('cluster')\n",
        "\n",
        "# Calcula as estatísticas descritivas\n",
        "stats = grouped.describe()\n",
        "\n",
        "# Imprime as estatísticas descritivas\n",
        "print(stats)\n"
      ],
      "metadata": {
        "id": "aQOt7e89xrYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary where the keys are the cluster labels and the values are lists of tuples representing the feature weights of each data point in that cluster\n",
        "clusters = {}\n",
        "for i, label in enumerate(labels):\n",
        "    if label not in clusters:\n",
        "        clusters[label] = []\n",
        "    clusters[label].append([(j, w) for j, w in enumerate(kmeans.cluster_centers_[label])])\n",
        "\n",
        "# calculate the mean weight for each feature in each cluster\n",
        "cluster_feature_weights = {}\n",
        "for label, tuples in clusters.items():\n",
        "    cluster_feature_weights[label] = {}\n",
        "    for i, attr in enumerate(['age', 'credit amount', 'duration']):\n",
        "        feature_sum = 0\n",
        "        for tpl in tuples:\n",
        "            feature_sum += tpl[i][1]\n",
        "        cluster_feature_weights[label][attr] = feature_sum / len(tuples)"
      ],
      "metadata": {
        "id": "bgB4e9EHi8le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3  # top n features\n",
        "top_features = {}\n",
        "for label, feature_weights in cluster_feature_weights.items():\n",
        "    sorted_features = sorted(feature_weights.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_features[label] = [f[0] for f in sorted_features[:n]]\n",
        "\n",
        "print(top_features)"
      ],
      "metadata": {
        "id": "2Hgh0UZkcaV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = ['credit amount', 'duration', 'age']\n",
        "# Get the centroids\n",
        "print(kmeans_final)"
      ],
      "metadata": {
        "id": "LgsfyduMcnQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = kmeans.cluster_centers_\n",
        "print(weights)\n",
        "\n",
        "# Create a bar chart for each cluster's feature weights\n",
        "x_pos = np.arange(len(feature_names))\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "    plt.bar(x_pos, weights[i], align='center')\n",
        "    plt.xticks(x_pos, feature_names)\n",
        "    plt.ylabel('Weight')\n",
        "    plt.title('Feature Weights for Cluster {}'.format(i+1))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qxdaAfjRerz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "male_count = []\n",
        "female_count = []\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "  male_count.append(((clusters_config['Sex'] == 'male') & (clusters_config['cluster'] == i)).sum())\n",
        "  female_count.append(((clusters_config['Sex'] == 'female') & (clusters_config['cluster'] == i)).sum())\n",
        "\n",
        "\n",
        "weights = list(zip(male_count,female_count))\n",
        "print(weights)\n",
        "feature_names = ['male','female']\n",
        "x_pos = np.arange(len(feature_names))\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "    plt.bar(x_pos, weights[i], align='center')\n",
        "    plt.xticks(x_pos, feature_names)\n",
        "    plt.ylabel('count')\n",
        "    plt.title('Sex count for Cluster {}'.format(i+1))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "cGxG6Co5ixq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = []\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "  weights.append((clusters_config['Age']).mean())\n",
        "\n",
        "\n",
        "\n",
        "print(clusters_config)\n",
        "\n",
        "\"\"\"\n",
        "x_pos = np.arange(len(feature_names))\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "    plt.bar(x_pos, weights[i], align='center')\n",
        "    plt.xticks(x_pos, feature_names)\n",
        "    plt.ylabel('count')\n",
        "    plt.title('Sex count for Cluster {}'.format(i+1))\n",
        "    plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8cxD7kXtnTET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "attributes_values = ['education','radio/TV','furniture/equipment','car']\n",
        "list_of_attributes = []\n",
        "#cria \n",
        "for i in range(len(attributes_values)):\n",
        "  list_of_attributes.append([])\n",
        "\n",
        "\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "  for j in range(len(attributes_values)):\n",
        "    list_of_attributes[j].append(((clusters_config['Purpose'] == attributes_values[j]) & (clusters_config['cluster'] == i)).sum())\n",
        "\n",
        "print(list_of_attributes)\n",
        "\n",
        "clusters = [[],[],[]]\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "  for j in range(len(attributes_values)):\n",
        "    clusters[i].append(list_of_attributes[j][i])\n",
        "\n",
        "print(clusters)\n",
        "weights = clusters\n",
        "x_pos = np.arange(len(attributes_values))\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "    plt.bar(x_pos, weights[i], align='center')\n",
        "    plt.xticks(x_pos, attributes_values)\n",
        "    plt.ylabel('count')\n",
        "    plt.title('Purpose count for Cluster {}'.format(i+1))\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "7FwopVYBsp65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "free_count = []\n",
        "own_count = []\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "  free_count.append(((clusters_config['Housing'] == 'free') & (clusters_config['cluster'] == i)).sum())\n",
        "  own_count.append(((clusters_config['Housing'] == 'own') & (clusters_config['cluster'] == i)).sum())\n",
        "\n",
        "\n",
        "weights = list(zip(free_count,own_count))\n",
        "print(weights)\n",
        "feature_names = ['free','own']\n",
        "x_pos = np.arange(len(feature_names))\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "    plt.bar(x_pos, weights[i], align='center')\n",
        "    plt.xticks(x_pos, feature_names)\n",
        "    plt.ylabel('count')\n",
        "    plt.title('Housing count for Cluster {}'.format(i+1))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "n4vn72wZwUL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes_values = ['little','moderate']\n",
        "list_of_attributes = []\n",
        "#cria \n",
        "for i in range(len(attributes_values)):\n",
        "  list_of_attributes.append([])\n",
        "\n",
        "\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "  for j in range(len(attributes_values)):\n",
        "    list_of_attributes[j].append(((clusters_config['Checking account'] == attributes_values[j]) & (clusters_config['cluster'] == i)).sum())\n",
        "\n",
        "print(list_of_attributes)\n",
        "\n",
        "clusters = [[],[],[]]\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "  for j in range(len(attributes_values)):\n",
        "    clusters[i].append(list_of_attributes[j][i])\n",
        "\n",
        "print(clusters)\n",
        "weights = clusters\n",
        "x_pos = np.arange(len(attributes_values))\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "    plt.bar(x_pos, weights[i], align='center')\n",
        "    plt.xticks(x_pos, attributes_values)\n",
        "    plt.ylabel('count')\n",
        "    plt.title('Checking account for Cluster {}'.format(i+1))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "-tyScsMQ0j0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes_values = ['little','moderate']\n",
        "list_of_attributes = []\n",
        "#cria \n",
        "for i in range(len(attributes_values)):\n",
        "  list_of_attributes.append([])\n",
        "\n",
        "\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "  for j in range(len(attributes_values)):\n",
        "    list_of_attributes[j].append(((clusters_config['Saving accounts'] == attributes_values[j]) & (clusters_config['cluster'] == i)).sum())\n",
        "\n",
        "print(list_of_attributes)\n",
        "\n",
        "clusters = [[],[],[]]\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "  for j in range(len(attributes_values)):\n",
        "    clusters[i].append(list_of_attributes[j][i])\n",
        "\n",
        "print(clusters)\n",
        "weights = clusters\n",
        "x_pos = np.arange(len(attributes_values))\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "    plt.bar(x_pos, weights[i], align='center')\n",
        "    plt.xticks(x_pos, attributes_values)\n",
        "    plt.ylabel('count')\n",
        "    plt.title('Saving account for Cluster {}'.format(i+1))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "2G-8SMTT2dbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes_values = [1,2,3]\n",
        "list_of_attributes = []\n",
        "#cria \n",
        "for i in range(len(attributes_values)):\n",
        "  list_of_attributes.append([])\n",
        "\n",
        "\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "  for j in range(len(attributes_values)):\n",
        "    list_of_attributes[j].append(((clusters_config['Job'] == attributes_values[j]) & (clusters_config['cluster'] == i)).sum())\n",
        "\n",
        "print(list_of_attributes)\n",
        "\n",
        "clusters = [[],[],[]]\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "  for j in range(len(attributes_values)):\n",
        "    clusters[i].append(list_of_attributes[j][i])\n",
        "\n",
        "print(clusters)\n",
        "weights = clusters\n",
        "x_pos = np.arange(len(attributes_values))\n",
        "n_clusters=3\n",
        "for i in range(n_clusters):\n",
        "    plt.bar(x_pos, weights[i], align='center')\n",
        "    plt.xticks(x_pos, attributes_values)\n",
        "    plt.ylabel('count')\n",
        "    plt.title('Jobs for Cluster {}'.format(i+1))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UQ5X6OJe2yBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_age = df.groupby('cluster')['Age'].mean()\n",
        "print(mean_age)\n",
        "mean_age = df.groupby('cluster')['Credit amount'].mean()\n",
        "print(mean_age)\n",
        "mean_age = df.groupby('cluster')['Duration'].mean()\n",
        "print(mean_age)\n"
      ],
      "metadata": {
        "id": "mYpJKxMA3BnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y8W0KGJK6Amz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}